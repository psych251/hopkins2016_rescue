---
title: "Replication of Study 1 by Hopkins, Weisberg, and Taylor (2016, Cognition)"
author: "Howard Chiu (howardchiu@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

<!-- [No abstract is needed.]  Each replication project will have a straightforward, no frills report of the study and results.  These reports will be publicly available as supplementary material for the aggregate report(s) of the project as a whole.  Also, to maximize project integrity, the intro and methods will be written and critiqued in advance of data collection.  Introductions can be just 1-2 paragraphs clarifying the main idea of the original study, the target finding for replication, and any other essential information.  It will NOT have a literature review -- that is in the original publication. You can write both the introduction and the methods in past tense. -->  

### Research Interests
I am currently a first-year PhD Student in the Graduate School of Education in the Developmental and Psychological Sciences program, and I am interested in using transdiagnostic longitudinal neuroimaging methods to understand the developmental trajectories of individuals within and across functional areas, so as to better understand how neural development relates to educational and other well-being outcomes.

This paper, and the seductive allure effect in general, is relevant to my interest in communicating scientific findings precisely and concisely. I might also be susceptible to this very effect in my own desire for explanations as I pivot from working as a school psychologist to neuroscience research as a PhD student.

### Description of stimuli and procedures
The original study aimed to investigate whether participants would prefer reductive explanations and whether this would differ by the science of the phenomenon. Horizontal explanations were defined as referring "only to the science from which the phenomenon itself was drawn (e.g., using biological language to explain a biological phenomenon)", while reductive explanations were defined as explanations which "included information from the next lower level in the hierarchy (e.g., using chemical language to explain a biological phenomenon)".

Participants were presented with horizontal and reductive explanations of phenomenon in 6 sciences: physics, chemistry, biology, neuroscience, psychology, and social sciences. The stimuli were carefully constructed so that the reductive information would not provide any explanatory power, allowing the authors to separate out a potential preference for explanations with a reductive form even in the absence of any helpful reductive content.

The original authors observed that there was a main effect of explanation level, where participants rated reductive explanations as of higher quality than horizontal explanations. They also observed that there was a relatively positive view of neuroscience combined with a relatively negative view of psychology, with information in these domains positively and negatively influencing the perceived quality of the reductive explanations respectively.

### Challenges
Participants may approach this task, and the explanations provided, with vastly different background knowledge, as was observed in the original study. As the replication sample will be crowd-sourced online via Prolific, participants may be motivated to complete the task as fast as possible rather than attempt to maximise their performance on the task, even with the instructional check at the start of the experiment and the attentional check in the middle of the Rating Explanations task.

### Link to GitHub repository and original paper
The repository for this rescue project is located at [https://github.com/psych251/hopkins2016_rescue](https://github.com/psych251/hopkins2016_rescue), and the original paper 

> Hopkins, E. J., Weisberg, D. S., & Taylor, J. C. V. (2016). The seductive allure is a reductive allure: People prefer scientific explanations that contain logically irrelevant reductive information. Cognition, 155, 67–76. https://doi.org/10.1016/j.cognition.2016.06.011

is hosted [here](https://github.com/psych251/hopkins2016_rescue/blob/main/original_paper/original_paper.pdf).

## Summary of prior replication attempt

<!-- Based on the prior write-up, describe any differences between the original and 1st replication in terms of methods, sample, sample size, and analysis. Note any potential problems such as exclusion rates, noisy data, or issues with analysis. -->

The primary research questions were "whether participants would prefer reductive explanations and whether this would differ by the science of the phenomenon".

The measure of interest was the mean rating of quality of reductive and horizontal explanations of various scientific findings provided to participants along a sliding scale ranging from 3 (very good) to -3 (very poor). 

The replication utilised the methodology exactly as reported in the original study for the Rating Explanations task, with the exception of using a sample completely recruited from MTurk and no undergraduates. The replication study also omitted tests of scientific literacy, reflective thinking, logical syllogisms, as well as perceptions of science (Sections 2.4.2 to 2.4.5 of the original study) to reduce completion time. 

There were a high number of participants who failed the attention checks (70 out of 163 participants) in the replication study.

However, even when the participants who had failed attention checks (either or both) were included in the analysis, the replication study did not manage to replicate the main effect of explanation quality, as well as the significant effect of explanation level.

The repository for the previous replication is hosted [here](https://github.com/psych251/hopkins2016/).

### Original Findings

The regression in the original study revealed a significant main effect of explanation level (as reported in Figure 2 of original paper): reductive explanations (_n_=131, _M_=1.26, _SD_=1.71) were rated signficantly higher on average than horizonal explanations (_n_=128, _M_=1.04, _SD_=1.88). Effect sizes (Cohen's _d_ for within-subjects tests) were only reported for pairwise differences between adjacent fields in the hierarchy of sciences (Figure 4 of original paper). 

Participants (20 MTurk workers and 40 undergraduates out of the complete recruited sample of 167 MTurk workers and 152 undergraduates) who failed attention checks were excluded from the sample (i.e. 60 out of the total recruited sample of 319 participants).

## Methods

### Power Analysis

<!-- Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size.  Considerations of feasibility for selecting planned sample size.-->

The key statistical test for the original study and the subsequent replications is the main fixed effect of condition on rating of explanation quality within the linear mixed-effects model.

No power analysis or sample size justification was provided in the original paper, and the replication sample utilised the same MTurk sample size (_n_=167) from the original experiment to preserve power across both experiments.

I conducted the power analysis as a t-test of difference between two independent means as a proxy due to the difficulty of estimating the influence of random effects on power.

As explanation level was a between-subject manipulation, the power calculation is conducted based on the number of items tested rather than the number of participants in the study. In this case, each participant rates 12 explanations, and thus the number of explanations rated in the reductive condition was 876, and the number of explanations rated in the horizontal condition was 888. The main analysis of a two-tailed t-test of two independent means thus gives us a post-hoc power of 72.9%. The post-hoc power analysis was conducted with G*Power 3.1.

For the replication study, the post-hoc power was 36.4% after excluding responses for participants who had failed either attention check. The mean rating of horizontal explanations was 1.58 (_SD_=1.35, _n_=936) and the mean rating of reductive explanations was 1.48 (_SD_=1.39, _n_=1020).

### Planned Sample

<!-- Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any. -->

Like the original and replication samples, English-speaking adult participants residing in the United States will be recruited online (via Prolific instead of MTurk). Participants who complete the task and pass both attention checks will be included in the final sample for analysis. Details about the attention checks can be found in the Procedure or Control sections below.

Due to the high number of exclusions in in both the original study (60 participants) and the replication study (70 participants), we will be building in a 20% buffer in our planned sample recruitment.

With a planned sample of 200 workers on Prolific, and a 20% exclusion rate for failed attention checks, that would leave us approximately 160 participants. With 80 participants in each condition completing 12 items each, the number of explanations rated in each of the two conditions would be 960. This will give us a planned power of 76.4%. 

If all the data collected from the 200 participants can be used in the analysis (yielding 1200 items rated in each condition), this will give us a planned power of 85%.

<!-- 2.5 times original sample size gives you 80% chance to reject 33% powered study -->

### Materials

<!-- All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article. -->

The detailed process of constructing the stimuli can be found in the original paper. All materials for the Rating Explanations task can be found in [Appendix A](https://www.sciencedirect.com/science/article/pii/S0010027716301585) of the original paper and will be used without modifications.

### Procedure	

<!-- Can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article. -->

The following procedure is quoted from the original article:

> All participants completed an online survey hosted by Qualtrics. The survey had six components: Rating Explanations, Science Literacy, Reflective Thinking, Logical Syllogisms, Perceptions of Science, and Demographic Information. All participants completed the Rating Explanations component first and the Demographic Information last. The other four components were presented in a random order after the Rating Explanations component. The Rating Explanations task used a 2 (Explanation Level: horizontal, reductive) x 2 (Quality: good, bad) x 6 (Science: physics, biology, chemistry, neuroscience, psychology, social science) design. To be consistent with previous studies of this effect (Weisberg et al., 2008, 2015), explanation level was between-subjects: Participants were randomly assigned to either the horizontal or reductive condition. Quality and science were within-subjects: All participants rated two explanations from each science, one good and one bad.

Unlike the original study, the participants will not be completing the Science Literacy, Reflective Thinking, Logical Syllogisms, and Perceptions of Science sections. They will thus only complete two components - the Rating Explanations, followed by Demographic Information.

#### Rating Explanations

The following paragraphs are quoted from the original article:

> All participants completed the Rating Explanations task first. For this task, participants used a sliding scale ranging from 3 to -3 to indicate their responses. They were first given instructions on how to use the slider; this also served as a check that participants were reading instructions. They were told to use the slider to select 0 on the first page in order to proceed with the survey. If they selected anything other than 0, they were directed to another page asking them again to select 0. Participants who did not select the correct response on this second page were excluded from analyses. 

> After these general instructions on using the slider, participants were given instructions for the explanations task (modified from Fernandez-Duque et al., 2015): 

>> You will now be presented with descriptions of various scientific findings. All the findings come from solid, replicable research; they are the kind of material you would encounter in a textbook. You will also read an explanation of each finding. Unlike the findings themselves, the explanations of the findings range in quality. Some explanations are better than others: They are more logically sound. Your job is to judge the quality of such explanations, which could range from very poor (-3) to very good (+3). 

> On each trial, participants were presented with a description of a scientific phenomenon, which was displayed for 10 s before participants could advance to the next screen. On the next screen, an explanation was displayed below the phenomenon, and participants were instructed to rate the quality of the explanation. Participants rated 12 explanations, with an attention check trial administered after the first six (Oppenheimer, Meyvis, & Davidenko, 2009). This trial was similar in format to the others. First, a description of a phenomenon was presented for 10 s. When participants advanced to the next screen, instead of seeing an explanation, they saw text instructing them to select 3 on the scale. Participants who did not select 3 were excluded from analyses. 

#### Demographics

The following procedure is quoted from the original article:

> At the end of the survey, participants answered a series of demographic questions, including gender, age, and year in school (for undergraduates) or highest degree completed (for MTurk workers). Participants from both samples were asked to pick the category that most closely matched the field of their highest degree (physical sciences, social sciences, engineering, humanities, health, and business) and to give the exact field. They were also asked whether they had taken any college- or graduate-level courses in anthropology, chemistry, physics, sociology, economics, neuroscience, psychology, political science, biology, or philosophy.

Since undergraduates will not be recruited for this sample, the demographic questions will only include gender, age, highest degree completed, category that closely matched the field of their highest degree (physical sciences, social sciences, engineering, humanities, health, and business), and college- or graduate-level coursework.

In addition, this rescue project not be asking about coursework in anthropology and philosophy as these subjects are tangential to the main research questions. Instead, the list of courses will have a combined social sciences item for anthropology/economics/philopsphy so that there will be 6 scientific disciplines in the list which are aligned to the rating explanations. (The original paper also averaged sociology, economics, and political science into a single post-hoc measure when analysing participants' perceptions of science as they found that the three domains were highly correlated.) This rescue study preserves the coursework question as the original study observed that "the total number of sciences (out of 6) in which a participant had taken courses was a significant predictor of overall difference scores in a linear regression: _F_ (1, 214) = 6.27, _p_ < 0.05, _R^2^_ = 0.02."

### Controls

<!-- What attention checks, positive or negative controls, or other quality control measures are you adding so that a (positive or negative) result will be more interpretable? -->

Both attention checks are placed within the Rating Explanations block of the survey.

The first attention check simply requires participants to select 0 on the sliding scale ranging from -3 to 3 after reading instructions on the Rating Explanations task. The second attention check occurs after participants rate 6 out of the 12 explanations. A phenomenon is also presented for 10 seconds, but "when participants advanced to the next screen, instead of seeing an explanation, they saw text instructing them to select 3" on the same sliding scale.

In addition, the description of each scientific phenomena was "displayed for 10 s before participants could advance to the next screen" to ensure that they had sufficient time to read before they could view the explanations provided. This delay should also ensure greater deliberation as participants could not rush through the questions as quickly.

The exact stimuli used for attention checks were not provided in the original paper, so the attention checks in the replication were used.

### Analysis Plan

<!-- Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.

**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do. -->

The following procedure for the key analysis of interest is quoted from the original article:

> Data from the explanations task were analyzed with a mixed-effects linear regression model (using the lme4 package in R) predicting the rating given on each trial from the sample (MTurk, undergraduates), the quality of the explanation (good, bad), the explanation level (horizontal, reductive), and the science from which the phenomenon was drawn (physics, chemistry, biology, neuroscience, psychology, and social science). Sample and explanation level were between-participants variables and quality and science were within-participants. All possible interactions between variables were tested.

Similar to the replication but not the original study, the between-participants variable "sample" will not be available for analysis as there will not be an undergraduate population recruited.

### Differences from Original Study and 1st replication

<!-- Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect. -->

Unlike the original study and replication, the sample will be recruited online via Prolific rather than Amazon Mechanical Turk (MTurk). The other differences have been detailed in the sections above.

Given that the experimental materials and procedure involved in testing the primary research questions remain identical, and the analysis plan is identical save for one between-participants variable which did not make a difference in the analysis in the original study, these adjustments to the original study are not expected to make a difference based on claims in the original article.

<!--
### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.
-->

#### Actual Sample
<!-- Sample size, demographics, data exclusions based on rules spelled out in analysis plan. -->

#### Differences from pre-data collection methods plan
<!-- Any differences from what was described as the original plan, or “none”. -->


## Results


### Data preparation

Data preparation following the analysis plan. Code has been kindly provided by the author of the replication study, and will mostly be used as is with some edits for debugging and naming conventions.

1. Load qualtRics, tidyverse, and lme4 libraries.
2. Read in data from original study, replication study, and current rescue study.
3. Remove columns unnecessary for primary analysis.
4. Select participants who passed both attention checks.
5. Reshape to extract summary statistics for included participants.
6. Convert ratings into integers and factor variables into categories (good and bad explanations, horizontal and reductive explanations).
7. Run 2x2x6 linear mixed-effects linear regression predicting explanation rating based on the quality of the explanation (good or bad), the explanation level (horizontal or reductive), and the science from which the phenomenon was drawn (6 sciences). 
8. Visualize plots.
9. Conduct exploratory analyses if needed.

```{r echo=TRUE}
### Data Preparation

#### Load Relevant Libraries and Functions
library(qualtRics) #To read data from Qualtrics
library(tidyverse) #To clean data and plot graphs in ggplot
library(lme4) #To run mixed-effects linear regression model in original study
library(lmerTest)
library(brms) #To run Bayesian supplementary analyses
# library(simr)

#### Import data
# raw_data <- read_survey("../data/pilotdata.csv")
# raw_data <- read_survey("../data/pilotb.csv")
raw_data <- read_survey("../data/fakedataformodelfit.csv")

#### Data exclusion / filtering
raw_data <- raw_data |> 
  select(-c('Feedback')) #delete columns not used in analysis

data_num <- raw_data |> 
  mutate(ID = 1:nrow(raw_data)) #add participant ID column

data_num <- data_num |> 
  select(Att_1, everything()) #move attention check 2 to front of data frame

#### Prepare data for analysis - create columns etc.
#convert to long format
data_long <- data_num |> 
  pivot_longer(cols = 'P1-HG_1':'S4-RB_1', names_to = "full_item", values_to = "rating", values_drop_na=TRUE)

#specify item attributes
data_long <- data_long |> 
  mutate(science = substr(full_item, start=1, stop=1))

data_long <- data_long |> 
  mutate(item = substr(full_item, start=1, stop=2))

data_long <- data_long |> 
  mutate(quality = substr(full_item, start=5, stop=5))
  
data_long <- data_long |> 
  select(-full_item)

data_long <- data_long |> 
  filter(rating!="")

#recode item names
data_long$quality <- ifelse(data_long$quality=="G", "good", "bad")

data_long$science <- ifelse(data_long$science=="P", "physics", data_long$science)
data_long$science <- ifelse(data_long$science=="C", "chemistry", data_long$science)
data_long$science <- ifelse(data_long$science=="B", "biology", data_long$science)
data_long$science <- ifelse(data_long$science=="N", "neuroscience", data_long$science)
data_long$science <- ifelse(data_long$science=="Y", "psychology", data_long$science)
data_long$science <- ifelse(data_long$science=="S", "social", data_long$science)

#convert columns to intended data types for analysis
data_long$rating <- as.integer(as.character(data_long$rating))

#exclude participants who failed attention check 1
# data_excluded_both <- data_long |> 
#  filter(Slider1_1==0)

#exclude participants who failed attention check 2
# data_excluded_both <- data_excluded_both |> 
#  filter(Att_1==3)
```

### Results of control measures

<!-- How did people perform on any quality control checks or positive and negative controls? -->

```{r echo=TRUE}
#total number of participants
unfiltered_rows = data_long %>%
  nrow()
unfiltered_total = (unfiltered_rows / 12)

#total number of participants who passed attention checks
# filtered_rows = data_excluded_both %>%
#  nrow()

# filtered_total = (filtered_rows / 12)
#number of excluded participants
# num_excluded = unfiltered_total - filtered_total

# num_excluded

#final number of included participants
# filtered_total
```

There was a high number of participants who failed the first attention check. I will need to relook at the formatting to emphasize that they should select 0.

### Confirmatory analysis

<!-- The analyses as specified in the analysis plan.  

*Three-panel graph with original, 1st replication, and your replication is ideal here* -->

```{r echo=TRUE}

#Read in and format data
# data <- data_excluded_both
data <- data_long

# Convert to factor
data$science <- as.factor(data$science)
data$quality <- as.factor(data$quality)
data$condition <- as.factor(data$Condition)

#Reorder levels of the factors to facilitate contrast coding in regressions:
data$science <- relevel(data$science, ref = "social")
data$science <- relevel(data$science, ref = "psychology")
data$science <- relevel(data$science, ref = "neuroscience")
data$science <- relevel(data$science, ref = "biology")
data$science <- relevel(data$science, ref = "chemistry")
data$science <- relevel(data$science, ref = "physics")

#Backward difference coding; each level is compared to the mean of the one before it 
contrasts(data$quality) <- matrix(c(-1/2,1/2), nrow=2)
contrasts(data$condition) <- matrix(c(-1/2,1/2), nrow=2)

#Deviation coding: each level is compared to the grand mean (physics is in the intercept)
contrasts(data$science) <- matrix(c(-1,1,0,0,0,0,-1,0,1,0,0,0,-1,0,0,1,0,0,-1,0,0,0,1,0,-1,0,0,0,0,1), nrow=6)

```

In the original experiment, participant, science and items are random effects, explanation level and quality are fixed effects, and participant rating is the dependent variable.

The original authors also note that "the bestfitting model included random intercepts by participant and item as well as a random effect of item on the slope for the quality variable".

This replication project does not have different samples of participants. Therefore, to the best of my understanding, the lmer equation is

```{r echo=TRUE}

# my_explanations <- lmer(rating ~ quality + condition + (quality * condition) + (1 | ID) + (1 | science) + (quality | item), data = data)

# summary(my_explanations)

lmert <- lmerTest::lmer(rating ~ condition + quality + science + condition:quality + condition:science + quality:science + (1 | ID) + (1 + quality | item), data = data)

summary(lmert)

# Perform power analysis for a specific contrast (e.g., 'quality' variable)
# power_analysis <- powerSim(lmert, test=fcompare(y~quality), nsim=5000)

# View power estimates
# print(power_analysis)
```


```{r}
# pwr1 <- powerCurve(lmert, nsim=100, along = "ID", test = fcompare(y~quality))
# plot(pwr1)
```


### Exploratory analyses

<!-- Any follow-up analyses desired (not required). -->

I repeated the analyses using Bayesian modelling, and obtained similar results as the lmer.

```{r echo=TRUE}

my_bayes_explanations <- brm(rating ~ condition + quality + science + condition:quality + condition:science + quality:science + (1 | ID) + (1 + quality | item), data = data)

# my_bayes_explanations <- brm(rating ~ quality + condition + (quality * condition) + (1 | ID) + (quality * condition | item), data = data)

print(my_bayes_explanations)
```
```{r echo=TRUE}

#read original data
original_data <- read.csv("../data/original_hopkins2016_data.csv", header = TRUE)

#relable science and explanation level values to their full names
original_data$science <- ifelse(original_data$science=="phys", "physics", original_data$science)
original_data$science <- ifelse(original_data$science=="chem", "chemistry", original_data$science)
original_data$science <- ifelse(original_data$science=="bio", "biology", original_data$science)
original_data$science <- ifelse(original_data$science=="neuro", "neuroscience", original_data$science)
original_data$science <- ifelse(original_data$science=="psych", "psychology", original_data$science)
original_data$science <- ifelse(original_data$science=="soc", "social", original_data$science)
original_data$condition <- ifelse(original_data$condition=="H", "Horizontal", original_data$condition)
original_data$condition <- ifelse(original_data$condition=="2", "Reductive", original_data$condition)

original_data$science <- factor(original_data$science, 
                  levels = c("physics", "chemistry", "biology", "neuroscience", "psychology", "social"),
                  labels = c("physics", "chemistry", "biology", "neuroscience", "psychology", "social"))
original_data$quality <- factor(original_data$quality, 
                  levels = c("good", "bad"),
                  labels = c("good", "bad"))

#group data frame by explanation quality, level, and scientific domain
original_grouped <- original_data %>%
  group_by(quality, condition, science) %>%
  summarise(avg_rating = mean(rating),
            sem_rating = sd(rating) / sqrt(n()))

#plot bar graph faceted by scientific domain
ggplot(original_grouped, aes(x=quality, y=avg_rating, fill=condition)) +
  geom_bar(position="dodge", stat="identity") + 
  scale_fill_brewer(palette="Set1") +
  geom_errorbar(aes(ymin = (avg_rating - sem_rating), ymax = (avg_rating + sem_rating)), position = "dodge") +
  facet_wrap(~science, nrow=1) +
  ggthemes::theme_few() +
  scale_y_continuous(name='explanation rating', limits=c(-3, 3), breaks=seq(-3, 3, 1)) +
  theme(strip.text = element_text(size = 8, margin = margin()))

```
This graph illustrates the results of the first replication. Code is not reproduced here to minimize clutter, but can be accessed at the GitHub repository linked above in the "Summary of prior replication attempt" section.
![Chuey (2019)](../data/firstrep.png)

```{r echo=TRUE}

#rescue graph
data <- data_long
replication_grouped <- data |> 
  group_by(quality, Condition, science) |> 
  summarise(avg_rating = mean(rating),
            sem_rating = sd(rating) / sqrt(n()))

replication_grouped$science <-   factor(replication_grouped$science, 
      levels = c("physics", "chemistry", "biology", "neuroscience", "psychology", "social"))

replication_grouped$quality <-
  factor(replication_grouped$quality,
         levels = c("good", "bad"))

ggplot(replication_grouped, aes(x=quality, y=avg_rating, fill=Condition)) +
  geom_bar(position="dodge", stat="identity") + 
  scale_fill_brewer(palette="Set1") +
  geom_errorbar(aes(ymin = (avg_rating - sem_rating), ymax = (avg_rating + sem_rating)), position = "dodge") +
  facet_wrap(~science, nrow=1) +
  ggthemes::theme_few() +
  scale_y_continuous(name='explanation rating', limits=c(-3, 3), breaks=seq(-3, 3, 1)) +
  theme(strip.text = element_text(size = 8, margin = margin()))

```


## Discussion

## Mini meta analysis
<!-- Combining across the original paper, 1st replication, and 2nd replication, what is the aggregate effect size? -->

### Summary of Replication Attempt

<!-- Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result. --> 

This replication with the pilot sample (without exclusions for failing attention checks) actually finds a main effect of explanation quality, as well as an interaction effect between explanation level and explanation quality. However, there is no main effect of explanation level.

### Commentary

<!-- Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long. -->

Although the 7-point Likert scale seems rudimentary, the same authors continued to use this measure for rating explanation quality, even in subsequent papers (Hopkins, Weisberg, & Taylor, 2019). A related study investigating the seductive allure of technical language (Silas et al., 2021) on explanations about how vaccination, immunisation and herd immunity work also utilised the same 7-point Likert scale.

<!-- Maybe talk about Lund University replication? -->

### References

Hopkins, E. J., Weisberg, D. S., & Taylor, J. C. V. (2019). Does expertise moderate the seductive allure of reductive explanations? Acta Psychologica, 198, 102890. https://doi.org/10.1016/j.actpsy.2019.102890

Silas, J., Jones, A., Weiss-Cohen, L., & Ayton, P. (2021). The seductive allure of technical language and its effect on covid-19 vaccine beliefs and intentions. Vaccine, 39(52), 7590–7597. https://doi.org/10.1016/j.vaccine.2021.11.027

https://lup.lub.lu.se/student-papers/record/8933160/file/8933162.pdf